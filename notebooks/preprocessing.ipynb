{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e601163",
   "metadata": {},
   "source": [
    "**T&D - Formatos de tabela**\n",
    "---------------\n",
    "O objetivo deste notebook é importar dados em formato JSON e utiliza-los para formar tabelas e comparar o tempo de salvamento e o tamanho daquele determinado formato.\n",
    "\n",
    "Serão comparados os tipos:\n",
    "- Delta\n",
    "- Hudi: Merge_on_read & ORC base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2540fd35",
   "metadata": {},
   "source": [
    "# Imports & Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e66ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0dce9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cwd = os.getcwd()\n",
    "parent_dir = os.path.dirname(cwd)\n",
    "os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aef034f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/17 21:10:39 WARN Utils: Your hostname, quero-Inspiron-15-3520 resolves to a loopback address: 127.0.1.1; using 192.168.15.151 instead (on interface wlp2s0)\n",
      "25/06/17 21:10:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/eric-yoshida/.ivy2/cache\n",
      "The jars for the packages stored in: /home/eric-yoshida/.ivy2/jars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/eric-yoshida/pyspark-env/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2fb5549f-28d7-4c3b-8e54-a57265c0d648;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.3.2 in central\n",
      "\tfound io.delta#delta-storage;3.3.2 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 181ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.3.2 from central in [default]\n",
      "\tio.delta#delta-storage;3.3.2 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-2fb5549f-28d7-4c3b-8e54-a57265c0d648\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/4ms)\n",
      "25/06/17 21:10:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder \n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"default\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e24c9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_path = \"./data/raw/\"\n",
    "\n",
    "kindle_df = spark.read.json(data_path + \"Kindle_Store_5.json.gz\")\n",
    "movies_df = spark.read.json(data_path + \"Movies_and_TV_5.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1882619b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields match: StructField('asin', StringType(), True)\n",
      "Fields match: StructField('image', ArrayType(StringType(), True), True)\n",
      "Fields match: StructField('overall', DoubleType(), True)\n",
      "Fields match: StructField('reviewText', StringType(), True)\n",
      "Fields match: StructField('reviewTime', StringType(), True)\n",
      "Fields match: StructField('reviewerID', StringType(), True)\n",
      "Fields match: StructField('reviewerName', StringType(), True)\n",
      "Fields do not match: StructField('style', StructType([StructField('Format:', StringType(), True)]), True) != StructField('style', StructType([StructField('Format:', StringType(), True), StructField('Shape:', StringType(), True), StructField('Size:', StringType(), True)]), True)\n",
      "Fields match: StructField('summary', StringType(), True)\n",
      "Fields match: StructField('unixReviewTime', LongType(), True)\n",
      "Fields match: StructField('verified', BooleanType(), True)\n",
      "Fields match: StructField('vote', StringType(), True)\n"
     ]
    }
   ],
   "source": [
    "for kindle_field, movie_field in zip(kindle_df.schema.fields, movies_df.schema.fields):\n",
    "    if kindle_field == movie_field:\n",
    "        print(f\"Fields match: {kindle_field}\")\n",
    "    else:\n",
    "        print(f\"Fields do not match: {kindle_field} != {movie_field}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dd9f155",
   "metadata": {},
   "outputs": [],
   "source": [
    "kindle_df = kindle_df.withColumn(\"style\", F.split(F.col(\"style.`Format:`\"), \",\").getItem(0))\n",
    "movies_df = movies_df.withColumn(\"style\", F.split(F.col(\"style.`Format:`\"), \",\").getItem(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1baa7baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>image</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>style</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>verified</th>\n",
       "      <th>vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0005089549</td>\n",
       "      <td>None</td>\n",
       "      <td>5.0</td>\n",
       "      <td>So sorry I didn't purchase this years ago when...</td>\n",
       "      <td>11 9, 2012</td>\n",
       "      <td>A2M1CU2IRZG0K9</td>\n",
       "      <td>Terri</td>\n",
       "      <td>VHS Tape</td>\n",
       "      <td>Amazing!</td>\n",
       "      <td>1352419200</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0005089549</td>\n",
       "      <td>None</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Believe me when I tell you that you will recei...</td>\n",
       "      <td>12 30, 2011</td>\n",
       "      <td>AFTUJYISOFHY6</td>\n",
       "      <td>Melissa D. Abercrombie</td>\n",
       "      <td>VHS Tape</td>\n",
       "      <td>Great Gospel VHS of the Cathedrals!</td>\n",
       "      <td>1325203200</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000503860X</td>\n",
       "      <td>None</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I have seen X live many times, both in the ear...</td>\n",
       "      <td>04 21, 2005</td>\n",
       "      <td>A3JVF9Y53BEOGC</td>\n",
       "      <td>Anthony Thompson</td>\n",
       "      <td>DVD</td>\n",
       "      <td>A great document of a great band</td>\n",
       "      <td>1114041600</td>\n",
       "      <td>True</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000503860X</td>\n",
       "      <td>None</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I was so excited for this!  Finally, a live co...</td>\n",
       "      <td>04 6, 2005</td>\n",
       "      <td>A12VPEOEZS1KTC</td>\n",
       "      <td>JadeRain</td>\n",
       "      <td>DVD</td>\n",
       "      <td>YES!!  X LIVE!!</td>\n",
       "      <td>1112745600</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000503860X</td>\n",
       "      <td>None</td>\n",
       "      <td>5.0</td>\n",
       "      <td>X is one of the best punk bands ever. I don't ...</td>\n",
       "      <td>12 3, 2010</td>\n",
       "      <td>ATLZNVLYKP9AZ</td>\n",
       "      <td>T. Fisher</td>\n",
       "      <td>DVD</td>\n",
       "      <td>X have still got it</td>\n",
       "      <td>1291334400</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>000503860X</td>\n",
       "      <td>None</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I've loved X since I first saw them here in Sa...</td>\n",
       "      <td>10 17, 2007</td>\n",
       "      <td>A3TNYNA2360NPA</td>\n",
       "      <td>Stanley  C. Sargent</td>\n",
       "      <td>DVD</td>\n",
       "      <td>A Revival to match the early shows by a great ...</td>\n",
       "      <td>1192579200</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>000503860X</td>\n",
       "      <td>None</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I must admit I was hesitant to purchase this D...</td>\n",
       "      <td>11 8, 2005</td>\n",
       "      <td>A2LUL6PRTXE7SE</td>\n",
       "      <td>PSM/Bokor</td>\n",
       "      <td>DVD</td>\n",
       "      <td>Old punks still rule...</td>\n",
       "      <td>1131408000</td>\n",
       "      <td>True</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0005419263</td>\n",
       "      <td>None</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The little ones love this</td>\n",
       "      <td>04 12, 2016</td>\n",
       "      <td>A2CFV9UPFTTM10</td>\n",
       "      <td>SuzieQ</td>\n",
       "      <td>Audio CD</td>\n",
       "      <td>Love it</td>\n",
       "      <td>1460419200</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0005419263</td>\n",
       "      <td>None</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Good songs. The DVD is a little boring.</td>\n",
       "      <td>07 28, 2014</td>\n",
       "      <td>A3139J3877Y61F</td>\n",
       "      <td>SingingButterfly</td>\n",
       "      <td>Audio CD</td>\n",
       "      <td>Three Stars</td>\n",
       "      <td>1406505600</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0005419263</td>\n",
       "      <td>None</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The DVD came in great condition and provided l...</td>\n",
       "      <td>12 21, 2010</td>\n",
       "      <td>A2PANT8U0OJNT4</td>\n",
       "      <td>Cutegirlmomma</td>\n",
       "      <td>DVD</td>\n",
       "      <td>Blast from the past</td>\n",
       "      <td>1292889600</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin image  overall  \\\n",
       "0  0005089549  None      5.0   \n",
       "1  0005089549  None      5.0   \n",
       "2  000503860X  None      5.0   \n",
       "3  000503860X  None      5.0   \n",
       "4  000503860X  None      5.0   \n",
       "5  000503860X  None      5.0   \n",
       "6  000503860X  None      5.0   \n",
       "7  0005419263  None      5.0   \n",
       "8  0005419263  None      3.0   \n",
       "9  0005419263  None      5.0   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  So sorry I didn't purchase this years ago when...   11 9, 2012   \n",
       "1  Believe me when I tell you that you will recei...  12 30, 2011   \n",
       "2  I have seen X live many times, both in the ear...  04 21, 2005   \n",
       "3  I was so excited for this!  Finally, a live co...   04 6, 2005   \n",
       "4  X is one of the best punk bands ever. I don't ...   12 3, 2010   \n",
       "5  I've loved X since I first saw them here in Sa...  10 17, 2007   \n",
       "6  I must admit I was hesitant to purchase this D...   11 8, 2005   \n",
       "7                          The little ones love this  04 12, 2016   \n",
       "8            Good songs. The DVD is a little boring.  07 28, 2014   \n",
       "9  The DVD came in great condition and provided l...  12 21, 2010   \n",
       "\n",
       "       reviewerID            reviewerName      style  \\\n",
       "0  A2M1CU2IRZG0K9                   Terri   VHS Tape   \n",
       "1   AFTUJYISOFHY6  Melissa D. Abercrombie   VHS Tape   \n",
       "2  A3JVF9Y53BEOGC        Anthony Thompson        DVD   \n",
       "3  A12VPEOEZS1KTC                JadeRain        DVD   \n",
       "4   ATLZNVLYKP9AZ               T. Fisher        DVD   \n",
       "5  A3TNYNA2360NPA     Stanley  C. Sargent        DVD   \n",
       "6  A2LUL6PRTXE7SE               PSM/Bokor        DVD   \n",
       "7  A2CFV9UPFTTM10                  SuzieQ   Audio CD   \n",
       "8  A3139J3877Y61F        SingingButterfly   Audio CD   \n",
       "9  A2PANT8U0OJNT4           Cutegirlmomma        DVD   \n",
       "\n",
       "                                             summary  unixReviewTime  \\\n",
       "0                                           Amazing!      1352419200   \n",
       "1                Great Gospel VHS of the Cathedrals!      1325203200   \n",
       "2                   A great document of a great band      1114041600   \n",
       "3                                    YES!!  X LIVE!!      1112745600   \n",
       "4                                X have still got it      1291334400   \n",
       "5  A Revival to match the early shows by a great ...      1192579200   \n",
       "6                            Old punks still rule...      1131408000   \n",
       "7                                            Love it      1460419200   \n",
       "8                                        Three Stars      1406505600   \n",
       "9                                Blast from the past      1292889600   \n",
       "\n",
       "   verified  vote  \n",
       "0      True  None  \n",
       "1      True  None  \n",
       "2      True    11  \n",
       "3      True     5  \n",
       "4      True     5  \n",
       "5      True  None  \n",
       "6      True    17  \n",
       "7      True  None  \n",
       "8      True  None  \n",
       "9      True  None  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d80552a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>image</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>style</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>verified</th>\n",
       "      <th>vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B000FA5KK0</td>\n",
       "      <td>None</td>\n",
       "      <td>4.0</td>\n",
       "      <td>pretty good story, a little exaggerated, but I...</td>\n",
       "      <td>07 3, 2014</td>\n",
       "      <td>A2LSKD2H9U8N0J</td>\n",
       "      <td>sandra sue marsolek</td>\n",
       "      <td>Kindle Edition</td>\n",
       "      <td>pretty good story</td>\n",
       "      <td>1404345600</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B000FA5KK0</td>\n",
       "      <td>None</td>\n",
       "      <td>5.0</td>\n",
       "      <td>If you've read other max brand westerns, you k...</td>\n",
       "      <td>05 26, 2014</td>\n",
       "      <td>A2QP13XTJND1QS</td>\n",
       "      <td>Tpl</td>\n",
       "      <td>Kindle Edition</td>\n",
       "      <td>A very good book</td>\n",
       "      <td>1401062400</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B000FA5KK0</td>\n",
       "      <td>None</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Love Max, always a fun twist</td>\n",
       "      <td>09 16, 2016</td>\n",
       "      <td>A8WQ7MAG3HFOZ</td>\n",
       "      <td>Alverne F. Anderson</td>\n",
       "      <td>Kindle Edition</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1473984000</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000FA5KK0</td>\n",
       "      <td>None</td>\n",
       "      <td>5.0</td>\n",
       "      <td>As usual for him, a good book</td>\n",
       "      <td>03 3, 2016</td>\n",
       "      <td>A1E0MODSRYP7O</td>\n",
       "      <td>Jeff</td>\n",
       "      <td>Kindle Edition</td>\n",
       "      <td>a good</td>\n",
       "      <td>1456963200</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B000FA5KK0</td>\n",
       "      <td>None</td>\n",
       "      <td>5.0</td>\n",
       "      <td>MB is one of the original western writers and ...</td>\n",
       "      <td>09 10, 2015</td>\n",
       "      <td>AYUTCGVSM1H7T</td>\n",
       "      <td>DEHS - EddyRapcon</td>\n",
       "      <td>Kindle Edition</td>\n",
       "      <td>A Western</td>\n",
       "      <td>1441843200</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B000FA5KK0</td>\n",
       "      <td>None</td>\n",
       "      <td>5.0</td>\n",
       "      <td>great book</td>\n",
       "      <td>06 14, 2015</td>\n",
       "      <td>A9DXGM6YGV14D</td>\n",
       "      <td>Linda DeWachter</td>\n",
       "      <td>Kindle Edition</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1434240000</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B000FA5KK0</td>\n",
       "      <td>None</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A good, solid Western - yes, a little contrive...</td>\n",
       "      <td>06 2, 2015</td>\n",
       "      <td>A3MF8G33UKQLGL</td>\n",
       "      <td>mzbeastle</td>\n",
       "      <td>Kindle Edition</td>\n",
       "      <td>Fast read, entertaining, like a 1930s Western ...</td>\n",
       "      <td>1433203200</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B000FA5KK0</td>\n",
       "      <td>None</td>\n",
       "      <td>4.0</td>\n",
       "      <td>ALMOST BEEN TOO LONG SINCE I READ IT. GOOD REA...</td>\n",
       "      <td>05 3, 2015</td>\n",
       "      <td>A12AIK6DSUF1EW</td>\n",
       "      <td>R. DAVIS</td>\n",
       "      <td>Kindle Edition</td>\n",
       "      <td>ANOTHER GOOD WESTERN</td>\n",
       "      <td>1430611200</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B000FA5KK0</td>\n",
       "      <td>None</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Enjoyed this book and will read more from this...</td>\n",
       "      <td>02 2, 2015</td>\n",
       "      <td>A2RW1CXT3XSLXW</td>\n",
       "      <td>Barbara Rigsby</td>\n",
       "      <td>Kindle Edition</td>\n",
       "      <td>Good</td>\n",
       "      <td>1422835200</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B000FA5KK0</td>\n",
       "      <td>None</td>\n",
       "      <td>4.0</td>\n",
       "      <td>A very good read for you Western fans.  Great ...</td>\n",
       "      <td>12 19, 2014</td>\n",
       "      <td>A2ID7H6SRK5XS0</td>\n",
       "      <td>Richard Johnson</td>\n",
       "      <td>Kindle Edition</td>\n",
       "      <td>Get it</td>\n",
       "      <td>1418947200</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin image  overall  \\\n",
       "0  B000FA5KK0  None      4.0   \n",
       "1  B000FA5KK0  None      5.0   \n",
       "2  B000FA5KK0  None      5.0   \n",
       "3  B000FA5KK0  None      5.0   \n",
       "4  B000FA5KK0  None      5.0   \n",
       "5  B000FA5KK0  None      5.0   \n",
       "6  B000FA5KK0  None      3.0   \n",
       "7  B000FA5KK0  None      4.0   \n",
       "8  B000FA5KK0  None      5.0   \n",
       "9  B000FA5KK0  None      4.0   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  pretty good story, a little exaggerated, but I...   07 3, 2014   \n",
       "1  If you've read other max brand westerns, you k...  05 26, 2014   \n",
       "2                       Love Max, always a fun twist  09 16, 2016   \n",
       "3                      As usual for him, a good book   03 3, 2016   \n",
       "4  MB is one of the original western writers and ...  09 10, 2015   \n",
       "5                                         great book  06 14, 2015   \n",
       "6  A good, solid Western - yes, a little contrive...   06 2, 2015   \n",
       "7  ALMOST BEEN TOO LONG SINCE I READ IT. GOOD REA...   05 3, 2015   \n",
       "8  Enjoyed this book and will read more from this...   02 2, 2015   \n",
       "9  A very good read for you Western fans.  Great ...  12 19, 2014   \n",
       "\n",
       "       reviewerID         reviewerName            style  \\\n",
       "0  A2LSKD2H9U8N0J  sandra sue marsolek   Kindle Edition   \n",
       "1  A2QP13XTJND1QS                  Tpl   Kindle Edition   \n",
       "2   A8WQ7MAG3HFOZ  Alverne F. Anderson   Kindle Edition   \n",
       "3   A1E0MODSRYP7O                 Jeff   Kindle Edition   \n",
       "4   AYUTCGVSM1H7T    DEHS - EddyRapcon   Kindle Edition   \n",
       "5   A9DXGM6YGV14D      Linda DeWachter   Kindle Edition   \n",
       "6  A3MF8G33UKQLGL            mzbeastle   Kindle Edition   \n",
       "7  A12AIK6DSUF1EW             R. DAVIS   Kindle Edition   \n",
       "8  A2RW1CXT3XSLXW       Barbara Rigsby   Kindle Edition   \n",
       "9  A2ID7H6SRK5XS0      Richard Johnson   Kindle Edition   \n",
       "\n",
       "                                             summary  unixReviewTime  \\\n",
       "0                                  pretty good story      1404345600   \n",
       "1                                   A very good book      1401062400   \n",
       "2                                         Five Stars      1473984000   \n",
       "3                                             a good      1456963200   \n",
       "4                                          A Western      1441843200   \n",
       "5                                         Five Stars      1434240000   \n",
       "6  Fast read, entertaining, like a 1930s Western ...      1433203200   \n",
       "7                               ANOTHER GOOD WESTERN      1430611200   \n",
       "8                                               Good      1422835200   \n",
       "9                                             Get it      1418947200   \n",
       "\n",
       "   verified  vote  \n",
       "0      True  None  \n",
       "1      True  None  \n",
       "2      True  None  \n",
       "3      True  None  \n",
       "4      True     2  \n",
       "5      True  None  \n",
       "6      True     3  \n",
       "7      True  None  \n",
       "8      True  None  \n",
       "9      True  None  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kindle_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c585393",
   "metadata": {},
   "outputs": [],
   "source": [
    "united_df = (\n",
    "    kindle_df\n",
    "    .union(movies_df)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebecf76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "united_df.write.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e709bb85",
   "metadata": {},
   "source": [
    "# Saving as Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c10ed7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder \n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"delta\")\n",
    "    .config(\"spark.jars.packages\",\n",
    "            \"io.delta:delta-core_2.12:3.3.2\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\",\n",
    "            \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9dcdba36",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[DELTA_CONFIGURE_SPARK_SESSION_WITH_EXTENSION_AND_CATALOG] This Delta operation requires the SparkSession to be configured with the\nDeltaSparkSessionExtension and the DeltaCatalog. Please set the necessary\nconfigurations when creating the SparkSession as shown below.\n\n  SparkSession.builder()\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n    ...\n    .getOrCreate()\n\nIf you are using spark-shell/pyspark/spark-submit, you can add the required configurations to the command as show below:\n--conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m delta_path = \u001b[33m\"\u001b[39m\u001b[33m./data/delta/united_data\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43munited_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdelta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moverwrite\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moverwriteSchema\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTrue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pyspark-env/lib/python3.12/site-packages/pyspark/sql/readwriter.py:1463\u001b[39m, in \u001b[36mDataFrameWriter.save\u001b[39m\u001b[34m(self, path, format, mode, partitionBy, **options)\u001b[39m\n\u001b[32m   1461\u001b[39m     \u001b[38;5;28mself\u001b[39m._jwrite.save()\n\u001b[32m   1462\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1463\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pyspark-env/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pyspark-env/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [DELTA_CONFIGURE_SPARK_SESSION_WITH_EXTENSION_AND_CATALOG] This Delta operation requires the SparkSession to be configured with the\nDeltaSparkSessionExtension and the DeltaCatalog. Please set the necessary\nconfigurations when creating the SparkSession as shown below.\n\n  SparkSession.builder()\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n    ...\n    .getOrCreate()\n\nIf you are using spark-shell/pyspark/spark-submit, you can add the required configurations to the command as show below:\n--conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\n"
     ]
    }
   ],
   "source": [
    "delta_path = \"./data/delta/united_data\"\n",
    "\n",
    "united_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"True\").save(delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86ef179",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c161c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/17 20:52:49 WARN Utils: Your hostname, quero-Inspiron-15-3520 resolves to a loopback address: 127.0.1.1; using 192.168.15.151 instead (on interface wlp2s0)\n",
      "25/06/17 20:52:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/eric-yoshida/pyspark-env/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/eric-yoshida/.ivy2/cache\n",
      "The jars for the packages stored in: /home/eric-yoshida/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-47d3ee2d-4240-47de-b64d-6c35f992eab4;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.3.2 in central\n",
      "\tfound io.delta#delta-storage;3.3.2 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 200ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.3.2 from central in [default]\n",
      "\tio.delta#delta-storage;3.3.2 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-47d3ee2d-4240-47de-b64d-6c35f992eab4\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/7ms)\n",
      "25/06/17 20:52:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder \n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"hudi\")\n",
    "    .config(\"spark.jars\", \"/home/eric-yoshida/.ivy2/jars/hudi-spark3.5-bundle_2.12-1.0.2.jar\")\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62a2b302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- image: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- style: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      " |-- verified: boolean (nullable = true)\n",
      " |-- vote: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "united_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b5bce30",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o249.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: hudi. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:260)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: hudi.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 16 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      1\u001b[39m hudi_path = \u001b[33m\"\u001b[39m\u001b[33m./data/hudi/united_data\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m (\n\u001b[32m      4\u001b[39m     \u001b[43munited_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhudi\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhoodie.datasource.write.table.type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMERGE_ON_READ\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhoodie.base.file.format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mORC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhoodie.datasource.write.recordkey.field\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreviewerID\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhoodie.datasource.write.precombine.field\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munixReviewTime\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moverwrite\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moverwriteSchema\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTrue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhudi_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m  )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pyspark-env/lib/python3.12/site-packages/pyspark/sql/readwriter.py:1463\u001b[39m, in \u001b[36mDataFrameWriter.save\u001b[39m\u001b[34m(self, path, format, mode, partitionBy, **options)\u001b[39m\n\u001b[32m   1461\u001b[39m     \u001b[38;5;28mself\u001b[39m._jwrite.save()\n\u001b[32m   1462\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1463\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pyspark-env/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pyspark-env/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pyspark-env/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o249.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: hudi. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:260)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: hudi.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 16 more\n"
     ]
    }
   ],
   "source": [
    "hudi_path = \"./data/hudi/united_data\"\n",
    "\n",
    "(\n",
    "    united_df.write\n",
    "        .format(\"hudi\")\n",
    "        .option(\"hoodie.datasource.write.table.type\", \"MERGE_ON_READ\")\n",
    "        .option(\"hoodie.base.file.format\", \"ORC\")\n",
    "        .option(\"hoodie.datasource.write.recordkey.field\", \"reviewerID\")\n",
    "        .option(\"hoodie.datasource.write.precombine.field\", \"unixReviewTime\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"overwriteSchema\", \"True\")\n",
    "        .save(hudi_path)\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad732425",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
